# -*- coding: utf-8 -*-
"""Image Classification from scratch Keras CIFAR10.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nDjgJpTxhxRr9you-WQuXVfF7ksdLiEf

### **Imports**
"""

from numpy.random import seed
seed(888)


import tensorflow
tensorflow.random.set_seed(404)

# Commented out IPython magic to ensure Python compatibility.
import os
import numpy as np
import tensorflow as tf
import datetime

import itertools

import keras
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation, Dropout

from tensorflow.keras.callbacks import TensorBoard
from datetime import datetime

from sklearn.metrics import confusion_matrix

from IPython.display import display
from keras.preprocessing.image import array_to_img
import matplotlib.pyplot as plt
# %matplotlib inline

"""## **Constants**"""

# names from website
LOD_DIR = 'tensorboard_cifar_logs'
LABEL_NAMES = ['Plane', 'Car', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']
IMAGE_WIDTH = 32
IMAGE_HEIGHT = 32
IMAGE_PIXEL = IMAGE_WIDTH * IMAGE_HEIGHT
COLOR_CHANNELS = 3
TOTAL_INPUT = IMAGE_PIXEL* COLOR_CHANNELS
VALIDATION_SIZE = 10000
SMALL_TRAIN_SIZE = 1000

"""## **Get Data**"""

(x_train_all, y_train_all), (x_test, y_test) = cifar10.load_data()

type(cifar10)

"""Explore Data"""

x_train_all[0]

pic = array_to_img(x_train_all[7])
display(pic)

y_train_all.shape

y_train_all[7][0]

LABEL_NAMES[y_train_all[7][0]]

plt.imshow(x_train_all[4])
# Adding label names
plt.xlabel(LABEL_NAMES[y_train_all[4][0]], fontsize=15)
plt.show()

plt.figure(figsize=[15,5])
for i in range(10):
  plt.subplot(1, 10, i+1)
  plt.imshow(x_train_all[i])
  plt.yticks([])
  plt.xticks([])
  # Adding label names
  plt.xlabel(LABEL_NAMES[y_train_all[i][0]], fontsize=15)

numof_images, x, y, c  = x_train_all.shape

print(f'images = {numof_images} \t| width = {x} \t|height={y} \t|chanel = {c}')

x_test.shape

"""# Preprocessing Data"""

# stored value of pixel is
x_train_all[0][0][0] # its 8bit unsigned int

x_train_all, x_test = x_train_all/ 255.0, x_test / 255.0 # 255 is the largest value of RGB combination

x_train_all = x_train_all.reshape(x_train_all.shape[0], TOTAL_INPUT)
x_train_all.shape

x_test.shape
x_test = x_test.reshape(x_test.shape[0], TOTAL_INPUT)
x_test.shape

# validation set
x_val = x_train_all[:VALIDATION_SIZE]
y_val = y_train_all[:VALIDATION_SIZE]
x_val.shape

# trainning set
x_train = x_train_all[VALIDATION_SIZE:]
y_train = y_train_all[VALIDATION_SIZE:]
x_train.shape

"""### **Create a small dataset(for illutration)**"""

x_train_xs = x_train[:SMALL_TRAIN_SIZE]
y_train_xs = y_train[:SMALL_TRAIN_SIZE]

"""## **Define Neural Network using keras**
---
"""

model_1 = Sequential([
                      Dense(units=128, input_dim=TOTAL_INPUT, activation='relu', name='m1_hidden1'),# input layer
                      Dense(units=64, activation='relu', name='m1_hidden2'), # input layer
                      Dense(16, activation='relu', name='m1_hidden3'),  # input layer
                      Dense(10, activation='softmax', name='m1_output') # output layer
])

# regulization 
model_2 = Sequential()
model_2.add(Dropout(0.2, seed=42, input_shape=(TOTAL_INPUT, ))) # dropout technique
model_2.add(Dense(128, activation='relu', name='m2_hidden1'))
model_2.add(Dense(64, activation='relu', name='m2_hidden2'))
model_2.add(Dense(16, activation='relu', name='m2_hidden3'))
model_2.add(Dense(10, activation='softmax', name='m2_output'))

model_3 = Sequential()
model_3.add(Dropout(0.2, seed=42, input_shape=(TOTAL_INPUT, ))) # dropout technique
model_3.add(Dense(128, activation='relu', name='m3_hidden_1'))
model_3.add(Dropout(0.2, seed=42))
model_3.add(Dense(64, activation='relu', name='m3_hidden2'))
model_3.add(Dense(16, activation='relu', name='m3_hidden3'))
model_3.add(Dense(10, activation='softmax', name='m3_output'))

type(model_1)

"""## **Complie Model**"""

model_2.compile(
    optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

model_3.compile(
    optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

model_1.compile(
    optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']
)

# layer description
model_1.summary()

model_2.summary()

first_lyaer = 32*32*3*128 + 128 # 128 bias
second_lyaer = 128*64 + 64 # 64 bias
third_layer = 64*16 + 16 # 16 bias
output_layer = 16*10+10 # 10 bias
print(f'first layer {first_lyaer}\t second layer {second_lyaer}\t third layer {third_layer}\t putput layer {output_layer}\t total {first_lyaer+second_lyaer+third_layer+output_layer}')

"""## **Tensorboard (visualizing learning)**"""

def get_tensorboard(model_name):
  folder_name = f'{model_name} at {datetime.now().strftime("%Y-%d-%m %H:%M")}'
  LOG_PATH = '/content/drive/My Drive/Machine Learning Neural Network/Image Processing'
  dir_path = os.path.join(LOG_PATH, folder_name)
  try:
    os.makedirs(dir_path)
  except OSError as err:
    print(err.strerror)
  else:
    print('Sucessfully created Folder')
  return TensorBoard(log_dir=dir_path)

"""## **Fit Model**"""

sample_per_batch = 1000

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # epochs using trainingmodel. for random accuracy
# nr_epochs = 150
# model_1.fit(x_train_xs, y_train_xs, batch_size=sample_per_batch, epochs=nr_epochs,
#             callbacks=get_tensorboard(model_name='model 1'), validation_schema=(x_val, y_val))

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # epochs using trainingmodel. for random accuracy
# nr_epochs = 150
# model_2.fit(x_train_xs, y_train_xs, batch_size=sample_per_batch, epochs=nr_epochs,
#             callbacks=get_tensorboard(model_name='model 2'), validation_schema=(x_val, y_val))

"""# Training Full Data"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # epochs using trainingmodel. for random accuracy
# nr_epochs = 100
# model_1.fit(x_train, y_train, batch_size=sample_per_batch, epochs=nr_epochs,
#             callbacks=get_tensorboard(model_name='model 1 XL'), validation_schema=(x_val, y_val))

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # epochs using trainingmodel. for random accuracy
# nr_epochs = 100
# model_2.fit(x_train, y_train, batch_size=sample_per_batch, epochs=nr_epochs,
#             callbacks=get_tensorboard(model_name='model 2 XL'), validation_schema=(x_val, y_val))

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # epochs using trainingmodel. for random accuracy
# nr_epochs = 100
# model_3.fit(x_train, y_train, batch_size=sample_per_batch, epochs=nr_epochs,
#             callbacks=get_tensorboard(model_name='model 3 XL'), validation_schema=(x_val, y_val))

"""## Prediction on individual image"""

x_val.shape

x_val[0]

test = np.expand_dims(x_val[0], axis=0)

test.shape

model_2.predict(test).sum()

model_2.predict(x_val)

model_2.predict(x_val).shape

model_2.predict_classes(test)

y_val

print(y_val)

for i in range(10):
  test_img = np.expand_dims(x_val[i], axis=0)
  predict_val = model_2.predict_classes(test_img)[0]
  print(f'actual value {y_val[i][0]} vs predict value {predict_val}')

"""# Evaluation"""

model_2.metrics_names

test_loss, test_Accuracy = model_2.evaluate(x_test, y_test)

"""# confusion metrics"""

predictions = model_2.predict_classes(x_test)
conf_metrics = confusion_matrix(y_true=y_test, y_pred=predictions)

conf_metrics

conf_metrics.shape
nr_cols = conf_metrics.shape[0]
nr_rows = conf_metrics.shape[1]

plt.figure(figsize=(7,7))
plt.imshow(conf_metrics, cmap=plt.cm.Greens)
plt.title("Confusion Metrics")
plt.ylabel('actual value')
plt.xlabel('predicted value')
tick_mark = np.arange(10)
plt.yticks(tick_mark, LABEL_NAMES)
plt.colorbar()

for i, j in itertools.product(range(nr_rows), range(nr_cols)):
  plt.text(i, j, conf_metrics[i, j], horizontalalignment='center',
           color='white' if conf_metrics[i,j]>conf_metrics.max()/2 else 'black')
plt.show()

# true positive
np.diag(conf_metrics)

recal = np.diag(conf_metrics)/np.sum(conf_metrics, axis=1)
recal

precision = np.diag(conf_metrics)/np.sum(conf_metrics, axis=0)
precision

avg_recall = np.mean(recal)
avg_preci = np.mean(precision)
print(f'model 2 recall score is {avg_recall}')
print(f'model 2 precision score is {avg_preci}')

f1_score = 2 * (avg_preci * avg_recall) / (avg_preci + avg_recall) 
print(f'f  score is {f1_score}')

